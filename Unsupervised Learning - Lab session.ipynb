{"cells":[{"cell_type":"markdown","id":"83f26a29","metadata":{"id":"83f26a29"},"source":["# Unsupervised Lab Session"]},{"cell_type":"markdown","id":"8ea571d1","metadata":{"id":"8ea571d1"},"source":["## Learning outcomes:\n","- Exploratory data analysis and data preparation for model building.\n","- PCA for dimensionality reduction.\n","- K-means and Agglomerative Clustering"]},{"cell_type":"markdown","id":"fd7f778a","metadata":{"id":"fd7f778a"},"source":["## Problem Statement\n","Based on the given marketing campigan dataset, segment the similar customers into suitable clusters. Analyze the clusters and provide your insights to help the organization promote their business."]},{"cell_type":"markdown","id":"33b58f8f","metadata":{"id":"33b58f8f"},"source":["## Context:\n","- Customer Personality Analysis is a detailed analysis of a company’s ideal customers. It helps a business to better understand its customers and makes it easier for them to modify products according to the specific needs, behaviors and concerns of different types of customers.\n","- Customer personality analysis helps a business to modify its product based on its target customers from different types of customer segments. For example, instead of spending money to market a new product to every customer in the company’s database, a company can analyze which customer segment is most likely to buy the product and then market the product only on that particular segment."]},{"cell_type":"markdown","id":"867166aa","metadata":{"id":"867166aa"},"source":["## About dataset\n","- Source: https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis?datasetId=1546318&sortBy=voteCount\n","\n","### Attribute Information:\n","- ID: Customer's unique identifier\n","- Year_Birth: Customer's birth year\n","- Education: Customer's education level\n","- Marital_Status: Customer's marital status\n","- Income: Customer's yearly household income\n","- Kidhome: Number of children in customer's household\n","- Teenhome: Number of teenagers in customer's household\n","- Dt_Customer: Date of customer's enrollment with the company\n","- Recency: Number of days since customer's last purchase\n","- Complain: 1 if the customer complained in the last 2 years, 0 otherwise\n","- MntWines: Amount spent on wine in last 2 years\n","- MntFruits: Amount spent on fruits in last 2 years\n","- MntMeatProducts: Amount spent on meat in last 2 years\n","- MntFishProducts: Amount spent on fish in last 2 years\n","- MntSweetProducts: Amount spent on sweets in last 2 years\n","- MntGoldProds: Amount spent on gold in last 2 years\n","- NumDealsPurchases: Number of purchases made with a discount\n","- AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise\n","- AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise\n","- AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise\n","- AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise\n","- AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise\n","- Response: 1 if customer accepted the offer in the last campaign, 0 otherwise\n","- NumWebPurchases: Number of purchases made through the company’s website\n","- NumCatalogPurchases: Number of purchases made using a catalogue\n","- NumStorePurchases: Number of purchases made directly in stores\n","- NumWebVisitsMonth: Number of visits to company’s website in the last month"]},{"cell_type":"markdown","id":"5a830406","metadata":{"id":"5a830406"},"source":["### 1. Import required libraries"]},{"cell_type":"code","execution_count":null,"id":"d65c5528","metadata":{"id":"d65c5528"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import silhouette_score\n"]},{"cell_type":"markdown","id":"c80eb960","metadata":{"id":"c80eb960"},"source":["### 2. Load the CSV file (i.e marketing.csv) and display the first 5 rows of the dataframe. Check the shape and info of the dataset."]},{"cell_type":"code","execution_count":null,"id":"1caebc10","metadata":{"id":"1caebc10"},"outputs":[],"source":["# Import required libraries\n","import pandas as pd\n","\n","# Load the CSV file\n","file_path = \"path/to/marketing.csv\"  # Replace with the actual path to your CSV file\n","df = pd.read_csv(file_path)\n","\n","# Display the first 5 rows of the dataframe\n","print(\"First 5 rows of the dataframe:\")\n","print(df.head())\n","\n","# Check the shape of the dataset\n","print(\"\\nShape of the dataset:\")\n","print(df.shape)\n","\n","# Display information about the dataset\n","print(\"\\nInfo of the dataset:\")\n","print(df.info())\n"]},{"cell_type":"markdown","id":"9ef75724","metadata":{"id":"9ef75724"},"source":["### 3. Check the percentage of missing values? If there is presence of missing values, treat them accordingly."]},{"cell_type":"code","execution_count":null,"id":"f2c231df","metadata":{"id":"f2c231df"},"outputs":[],"source":["# Check the percentage of missing values in each column\n","missing_percentage = df.isnull().mean() * 100\n","\n","# Display columns with missing values and their respective percentages\n","columns_with_missing_values = missing_percentage[missing_percentage > 0]\n","if not columns_with_missing_values.empty:\n","    print(\"Columns with missing values and their respective percentages:\")\n","    print(columns_with_missing_values)\n","else:\n","    print(\"No missing values in the dataset.\")\n","\n","df_filled = df.fillna(df.mean())  "]},{"cell_type":"markdown","id":"86f3709e","metadata":{"id":"86f3709e"},"source":["### 4. Check if there are any duplicate records in the dataset? If any drop them."]},{"cell_type":"code","execution_count":null,"id":"2970671a","metadata":{"id":"2970671a"},"outputs":[],"source":["# Check for duplicate records\n","duplicate_rows = df[df.duplicated()]\n","\n","if not duplicate_rows.empty:\n","    print(\"Duplicate records found. Dropping duplicates...\")\n","    # Drop duplicate records\n","    df = df.drop_duplicates()\n","    print(\"Duplicates dropped. Updated shape of the dataset:\", df.shape)\n","else:\n","    print(\"No duplicate records found in the dataset.\")\n"]},{"cell_type":"markdown","id":"3a6f2b5a","metadata":{"id":"3a6f2b5a"},"source":["### 5. Drop the columns which you think redundant for the analysis "]},{"cell_type":"code","execution_count":null,"id":"a9ca818b","metadata":{"id":"a9ca818b"},"outputs":[],"source":["# List of columns to be dropped (replace with the actual column names)\n","columns_to_drop = ['ID', 'Dt_Customer']\n","\n","# Drop the specified columns\n","df = df.drop(columns=columns_to_drop)\n","\n","# Display the updated shape of the dataset\n","print(\"Updated shape of the dataset after dropping columns:\", df.shape)\n"]},{"cell_type":"markdown","id":"4ff0a112","metadata":{"id":"4ff0a112"},"source":["### 6. Check the unique categories in the column 'Marital_Status'\n","- i) Group categories 'Married', 'Together' as 'relationship'\n","- ii) Group categories 'Divorced', 'Widow', 'Alone', 'YOLO', and 'Absurd' as 'Single'."]},{"cell_type":"code","execution_count":null,"id":"eb1be519","metadata":{"id":"eb1be519"},"outputs":[],"source":["# Check unique categories in the 'Marital_Status' column\n","unique_categories = df['Marital_Status'].unique()\n","print(\"Unique categories in 'Marital_Status':\", unique_categories)\n","\n","# Group categories 'Married' and 'Together' as 'relationship'\n","df['Marital_Status'] = df['Marital_Status'].replace(['Married', 'Together'], 'relationship')\n","\n","# Group categories 'Divorced', 'Widow', 'Alone', 'YOLO', and 'Absurd' as 'Single'\n","df['Marital_Status'] = df['Marital_Status'].replace(['Divorced', 'Widow', 'Alone', 'YOLO', 'Absurd'], 'Single')\n","\n","# Check the unique categories after grouping\n","updated_categories = df['Marital_Status'].unique()\n","print(\"\\nUnique categories after grouping:\")\n","print(updated_categories)\n"]},{"cell_type":"markdown","id":"9566bfbe","metadata":{"id":"9566bfbe"},"source":["### 7. Group the columns 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', and 'MntGoldProds' as 'Total_Expenses'"]},{"cell_type":"code","execution_count":null,"id":"3c3fa800","metadata":{"id":"3c3fa800"},"outputs":[],"source":["# List of columns to be grouped\n","expense_columns = ['MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\n","\n","# Create a new column 'Total_Expenses' by summing the specified columns\n","df['Total_Expenses'] = df[expense_columns].sum(axis=1)\n","\n","# Drop the individual expense columns if needed\n","df = df.drop(columns=expense_columns)\n","\n","# Display the updated dataframe\n","print(\"Updated dataframe with 'Total_Expenses':\")\n","print(df.head())\n"]},{"cell_type":"markdown","id":"bf0cd083","metadata":{"id":"bf0cd083"},"source":["### 8. Group the columns 'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', and 'NumDealsPurchases' as 'Num_Total_Purchases'"]},{"cell_type":"code","execution_count":null,"id":"9c535ede","metadata":{"id":"9c535ede"},"outputs":[],"source":["# List of columns to be grouped\n","purchase_columns = ['NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases', 'NumDealsPurchases']\n","\n","# Create a new column 'Num_Total_Purchases' by summing the specified columns\n","df['Num_Total_Purchases'] = df[purchase_columns].sum(axis=1)\n","\n","# Drop the individual purchase columns if needed\n","df = df.drop(columns=purchase_columns)\n","\n","# Display the updated dataframe\n","print(\"Updated dataframe with 'Num_Total_Purchases':\")\n","print(df.head())\n"]},{"cell_type":"markdown","id":"52d2dca5","metadata":{"id":"52d2dca5"},"source":["### 9. Group the columns 'Kidhome' and 'Teenhome' as 'Kids'"]},{"cell_type":"code","execution_count":null,"id":"f7c861a1","metadata":{"id":"f7c861a1"},"outputs":[],"source":["# List of columns to be grouped\n","kids_columns = ['Kidhome', 'Teenhome']\n","\n","# Create a new column 'Kids' by summing the specified columns\n","df['Kids'] = df[kids_columns].sum(axis=1)\n","\n","# Drop the individual columns 'Kidhome' and 'Teenhome' if needed\n","df = df.drop(columns=kids_columns)\n","\n","# Display the updated dataframe\n","print(\"Updated dataframe with 'Kids':\")\n","print(df.head())\n"]},{"cell_type":"markdown","id":"36f67474","metadata":{"id":"36f67474"},"source":["### 10. Group columns 'AcceptedCmp1 , 2 , 3 , 4, 5' and 'Response' as 'TotalAcceptedCmp'"]},{"cell_type":"code","execution_count":null,"id":"ecc9109f","metadata":{"id":"ecc9109f"},"outputs":[],"source":["# List of columns to be grouped\n","cmp_columns = ['AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']\n","\n","# Create a new column 'TotalAcceptedCmp' by summing the specified columns\n","df['TotalAcceptedCmp'] = df[cmp_columns].sum(axis=1)\n","\n","# Drop the individual columns if needed\n","df = df.drop(columns=cmp_columns)\n","\n","# Display the updated dataframe\n","print(\"Updated dataframe with 'TotalAcceptedCmp':\")\n","print(df.head())\n"]},{"cell_type":"markdown","id":"886bfb08","metadata":{"id":"886bfb08"},"source":["### 11. Drop those columns which we have used above for obtaining new features"]},{"cell_type":"code","execution_count":null,"id":"e853e663","metadata":{"id":"e853e663"},"outputs":[],"source":["# List of columns used for creating new features\n","columns_to_drop = ['ID', 'Dt_Customer', 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n","                   'MntSweetProducts', 'MntGoldProds', 'NumWebPurchases', 'NumCatalogPurchases',\n","                   'NumStorePurchases', 'NumDealsPurchases', 'Kidhome', 'Teenhome',\n","                   'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'Response']\n","\n","# Drop the specified columns\n","df = df.drop(columns=columns_to_drop)\n","\n","# Display the updated dataframe\n","print(\"Updated dataframe after dropping columns:\")\n","print(df.head())\n"]},{"cell_type":"markdown","id":"4225ced7","metadata":{"id":"4225ced7"},"source":["### 12. Extract 'age' using the column 'Year_Birth' and then drop the column 'Year_birth'"]},{"cell_type":"code","execution_count":null,"id":"d517611e","metadata":{"id":"d517611e"},"outputs":[],"source":["# Extract 'age' from 'Year_Birth'\n","current_year = pd.to_datetime('today').year\n","df['Age'] = current_year - df['Year_Birth']\n","\n","# Drop the 'Year_Birth' column\n","df = df.drop(columns=['Year_Birth'])\n","\n","# Display the updated dataframe\n","print(\"Updated dataframe with 'Age' and without 'Year_Birth':\")\n","print(df.head())\n"]},{"cell_type":"markdown","id":"f2d3c92d","metadata":{"id":"f2d3c92d"},"source":["### 13. Encode the categorical variables in the dataset"]},{"cell_type":"code","execution_count":null,"id":"030cfc32","metadata":{"id":"030cfc32"},"outputs":[],"source":["# One-hot encode categorical variables\n","df_encoded = pd.get_dummies(df, columns=['Education', 'Marital_Status'], drop_first=True)\n","\n","# Display the updated dataframe with encoded categorical variables\n","print(\"Updated dataframe with encoded categorical variables:\")\n","print(df_encoded.head())\n"]},{"cell_type":"markdown","id":"9242e36d","metadata":{"id":"9242e36d"},"source":["### 14. Standardize the columns, so that values are in a particular range"]},{"cell_type":"code","execution_count":null,"id":"72475b68","metadata":{"id":"72475b68"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","# Select only the numerical columns for standardization\n","numerical_columns = df_encoded.select_dtypes(include=['float64', 'int64']).columns\n","\n","# Create a StandardScaler object\n","scaler = StandardScaler()\n","\n","# Fit and transform the selected numerical columns\n","df_encoded[numerical_columns] = scaler.fit_transform(df_encoded[numerical_columns])\n","\n","# Display the updated dataframe with standardized numerical columns\n","print(\"Updated dataframe with standardized numerical columns:\")\n","print(df_encoded.head())\n"]},{"cell_type":"markdown","id":"d063d2e2","metadata":{"id":"d063d2e2"},"source":["### 15. Apply PCA on the above dataset and determine the number of PCA components to be used so that 90-95% of the variance in data is explained by the same."]},{"cell_type":"code","execution_count":null,"id":"6df3c70e","metadata":{"id":"6df3c70e"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# Separate features (X) and target variable (if any)\n","# Assuming the target variable is not included in the dataframe\n","X = df_encoded.drop(columns=['target_variable_column'])  # Replace with the actual target variable column if present\n","\n","# Choose the desired explained variance threshold (e.g., 0.90 or 0.95)\n","desired_variance_threshold = 0.95\n","\n","# Create a PCA object\n","pca = PCA()\n","\n","# Fit PCA on the data\n","pca.fit(X)\n","\n","# Determine the number of components needed to explain the desired variance\n","cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n","num_components_needed = np.argmax(cumulative_variance >= desired_variance_threshold) + 1\n","\n","# Transform the data using the selected number of components\n","X_pca = pca.transform(X)[:, :num_components_needed]\n","\n","# Display the number of components and the cumulative explained variance\n","print(f\"Number of components needed: {num_components_needed}\")\n","print(f\"Cumulative explained variance with {num_components_needed} components: {cumulative_variance[num_components_needed - 1]:.4f}\")\n","\n","# Use X_pca for further analysis or modeling\n"]},{"cell_type":"markdown","id":"b2df19d7","metadata":{"id":"b2df19d7"},"source":["### 16. Apply K-means clustering and segment the data (Use PCA transformed data for clustering)"]},{"cell_type":"code","execution_count":null,"id":"a3a8bb4c","metadata":{"id":"a3a8bb4c"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","\n","# Assuming X_pca is the PCA-transformed data from the previous step\n","\n","# Choose the number of clusters (K) - You may need to tune this based on analysis\n","num_clusters = 3\n","\n","# Create a KMeans object\n","kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n","\n","# Fit the model to the PCA-transformed data\n","kmeans.fit(X_pca)\n","\n","# Assign cluster labels to the original dataframe\n","df_encoded['Cluster_Labels'] = kmeans.labels_\n","\n","# Display the cluster labels in the dataframe\n","print(\"Dataframe with Cluster Labels:\")\n","print(df_encoded.head())\n"]},{"cell_type":"markdown","id":"d8463aed","metadata":{"id":"d8463aed"},"source":["### 17. Apply Agglomerative clustering and segment the data (Use Original data for clustering), and perform cluster analysis by doing bivariate analysis between the cluster label and different features and write your observations."]},{"cell_type":"code","execution_count":null,"id":"b5ca165b","metadata":{"id":"b5ca165b"},"outputs":[],"source":["from sklearn.cluster import AgglomerativeClustering\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Assuming df_encoded is the dataframe after all the preprocessing steps\n","\n","# Choose the number of clusters (K) for Agglomerative clustering\n","num_clusters_agg = 3\n","\n","# Create an AgglomerativeClustering object\n","agg_clustering = AgglomerativeClustering(n_clusters=num_clusters_agg)\n","\n","# Fit the model to the original data\n","agg_labels = agg_clustering.fit_predict(df_encoded.drop(columns=['Cluster_Labels']))\n","\n","# Assign cluster labels to the dataframe\n","df_encoded['Agg_Cluster_Labels'] = agg_labels\n","\n","# Display the dataframe with Agglomerative cluster labels\n","print(\"Dataframe with Agglomerative Cluster Labels:\")\n","print(df_encoded.head())\n","\n","# Bivariate analysis between cluster labels and different features\n","features_for_analysis = ['Income', 'Total_Expenses', 'Num_Total_Purchases', 'Kids', 'TotalAcceptedCmp', 'Age']\n","\n","for feature in features_for_analysis:\n","    plt.figure(figsize=(10, 6))\n","    sns.boxplot(x='Agg_Cluster_Labels', y=feature, data=df_encoded)\n","    plt.title(f'Bivariate Analysis: {feature} by Agglomerative Cluster Labels')\n","    plt.show()\n"]},{"cell_type":"markdown","id":"797a5ecd","metadata":{"id":"797a5ecd"},"source":["### Visualization and Interpretation of results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Assuming df_encoded is the dataframe with Cluster_Labels from K-means clustering\n","\n","# Scatter plot for two selected features\n","plt.figure(figsize=(10, 6))\n","sns.scatterplot(x='Feature1', y='Feature2', hue='Cluster_Labels', data=df_encoded, palette='viridis', s=80)\n","plt.title('K-means Clustering Results')\n","plt.xlabel('Feature1')\n","plt.ylabel('Feature2')\n","plt.legend(title='Cluster Labels')\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"id":"d1e75760","metadata":{"id":"d1e75760"},"outputs":[],"source":["# Assuming df_encoded is the dataframe with Cluster_Labels from K-means clustering\n","\n","# Group by cluster labels and calculate mean for each feature\n","cluster_means = df_encoded.groupby('Cluster_Labels').mean()\n","\n","# Display the cluster means\n","print(\"Cluster Means:\")\n","print(cluster_means)\n"]},{"cell_type":"markdown","id":"36afd95b","metadata":{"id":"36afd95b"},"source":["-----\n","## Happy Learning\n","-----"]}],"metadata":{"colab":{"collapsed_sections":["36afd95b"],"name":"Unsupervised Learning - Lab session.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}
